{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mhkit\n",
    "from mhkit.wave import resource, performance, graphics\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from mhkit.wave.io import ndbc\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors \n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fc7d1d298f0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "import geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = 'swden'\n",
    "buoy_number = '46050' \n",
    "ndbc_available_data= ndbc.available_data(parameter, buoy_number)\n",
    "years_of_interest = ndbc_available_data[ndbc_available_data.year < 2013]\n",
    "filenames= years_of_interest['filename']\n",
    "ndbc_requested_data = ndbc.request_data(parameter, filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndbc_data={}\n",
    "# Create a Datetime Index and remove NOAA date columns for each year\n",
    "for year in ndbc_requested_data:\n",
    "    year_data = ndbc_requested_data[year]\n",
    "    ndbc_data[year] = ndbc.to_datetime_index(parameter, year_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = resource.energy_flux(ndbc_data['1996'].T,h=399.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize empty lists to store the results from each year\n",
    "Hm0_list=[]\n",
    "Te_list=[]\n",
    "J_list=[]\n",
    "Tp_list=[]\n",
    "\n",
    "# Iterate over each year and save the result in the initalized dictionary\n",
    "for year in ndbc_data:\n",
    "    year_data = ndbc_data[year]\n",
    "    Hm0_list.append(resource.significant_wave_height(year_data.T))\n",
    "    Te_list.append(resource.energy_period(year_data.T))\n",
    "    J_list.append(resource.energy_flux(year_data.T, h=399.))\n",
    "    Tp_list.append(resource.peak_period(year_data.T))\n",
    "\n",
    "# Concatenate list of Series into a single DataFrame\n",
    "Te = pd.concat(Te_list ,axis=0)\n",
    "Tp = pd.concat(Tp_list ,axis=0)\n",
    "Hm0 = pd.concat(Hm0_list ,axis=0)\n",
    "J = pd.concat(J_list ,axis=0)\n",
    "data = pd.concat([Hm0, Te, Tp, J],axis=1)\n",
    "\n",
    "# Drop any NaNs created from the calculation of Hm0 or Te\n",
    "data.dropna(inplace=True)\n",
    "# Sort the DateTime index\n",
    "data.sort_index(inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annual Scatter Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Hm0 Outliers\n",
    "data_clean = data[data.Hm0 < 20]\n",
    "sigma = data_clean.J.std()\n",
    "data_clean = data_clean[data_clean.J > (data_clean.J.mean() - 0.9* sigma)]\n",
    "#data_clean = data_clean[data_clean.J < data_clean.J.mean() + 3* data_clean.J.std()]\n",
    "\n",
    "Hm0_bin_size = 0.5\n",
    "Hm0_edges = np.arange(0,15+Hm0_bin_size,Hm0_bin_size)\n",
    "\n",
    "\n",
    "Te_bin_size = 1\n",
    "Te_edges = np.arange(0, 20+Te_bin_size,Te_bin_size)\n",
    "Te_edges\n",
    "\n",
    "h= plt.hist2d(data_clean.Te,  \n",
    "              data_clean.Hm0, \n",
    "              bins = (Te_edges,Hm0_edges), \n",
    "              norm = colors.LogNorm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wave Power by Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months=data_clean.index.month\n",
    "data_group=data_clean.groupby(months)\n",
    "# 3 plots\n",
    "QoIs = data_clean.keys()\n",
    "fig, axs = plt.subplots(len(QoIs),1, figsize=(8, 6), sharex=True)\n",
    "#shade between 25% and 75%\n",
    "QoIs = data_clean.keys()\n",
    "for i in range(len(QoIs)):\n",
    "    QoI = QoIs[i]\n",
    "    axs[i].plot(data_group.median()[QoI], marker='.')\n",
    "\n",
    "    axs[i].fill_between(months.unique(),\n",
    "                        data_group.describe()[QoI,   '25%'],\n",
    "                        data_group.describe()[QoI,   '75%'],\n",
    "                        alpha=0.2)\n",
    "\n",
    "plt.setp(axs[3], xlabel='Month')\n",
    "plt.setp(axs[0], ylabel='Hm0 [m]')\n",
    "\n",
    "plt.setp(axs[1], ylabel='Te  [s]')\n",
    "plt.setp(axs[2], ylabel='J   [kW/m]')\n",
    "plt.setp(axs[3], ylabel='Tp  [s]')\n",
    "\n",
    "#significant steepness\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumSum={}\n",
    "for month in data_clean.index.month.unique():    \n",
    "    F = mhkit.river.resource.exceedance_probability(data_clean[data_clean.index.month==month].J)\n",
    "    cumSum[month] = 1-F/100\n",
    "    cumSum[month].sort_values('F', inplace=True)\n",
    "plt.figure(figsize=(12,8) )\n",
    "for month in data_clean.index.month.unique():\n",
    "    plt.semilogx(data_clean.loc[cumSum[month].index].J, cumSum[month].F, '--', label=calendar.month_abbr[month])\n",
    "\n",
    "F = mhkit.river.resource.exceedance_probability(data_clean.J)\n",
    "F.sort_values('F', inplace=True)\n",
    "plt.semilogx(data_clean.loc[F.index].J, 1-F/100, 'k-', fillstyle='none', label='All')\n",
    "\n",
    "plt.xlim([1000, 1E6])    \n",
    "plt.grid()\n",
    "plt.xlabel('Energy Flux')\n",
    "plt.ylabel('Cumulative Distribution')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndbc_data['1996'] = ndbc_data['1996'][ndbc_data['1996'] != 999.0].dropna()\n",
    "ndbc_data['1996'].to_csv('46050_1996.csv')\n",
    "ndbc_data['1996'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Gaussian Mixture Model for each number of clusters\n",
    "Ns= [4, 8, 16, 32, 64]\n",
    "X = np.vstack((data_clean.Te.values, data_clean.Hm0.values)).T\n",
    "fig, axs = plt.subplots(len(Ns),1, figsize=(8, 24), sharex=True)\n",
    "\n",
    "results={}\n",
    "for N in Ns:\n",
    "    gmm = GaussianMixture(n_components=N).fit(X)\n",
    "\n",
    "    # Save centers and weights\n",
    "    result = pd.DataFrame(gmm.means_, columns=['Te','Hm0'])\n",
    "    result['weights'] = gmm.weights_\n",
    "\n",
    "    result['Tp'] = result.Te / 0.858\n",
    "    results[N] = result\n",
    "    \n",
    "    \n",
    "    labels = gmm.predict(X)\n",
    "    \n",
    "    i = Ns.index(N)\n",
    "    axs[i].scatter(data_clean.Te.values, data_clean.Hm0.values, c=labels, s=40)\n",
    "    axs[i].plot(result.Te, result.Hm0, 'm+')\n",
    "    axs[i].title.set_text(f'{N} Clusters')\n",
    "    plt.setp(axs[i], ylabel='Energy Period, $T_e$ [s]')\n",
    "plt.setp(axs[len(Ns)-1], xlabel='Sig. wave height, $Hm0$ [m')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Energy Flux\n",
    "\n",
    "Calculate energy flux for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = ndbc_data[year].columns.values\n",
    "f = w / 2*np.pi\n",
    "\n",
    "\n",
    "for N in results:\n",
    "    result = results[N]\n",
    "    J=[]\n",
    "    for i in range(len(result)):\n",
    "        b = resource.bretschneider_spectrum(f, result.Tp[i], result.Hm0[i])\n",
    "        J.extend([resource.energy_flux(b, h=399.).values[0][0]])\n",
    "    \n",
    "    result['J']  = J\n",
    "    results[N] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hm0_max=14\n",
    "Te_max=16\n",
    "\n",
    "reduce_factor=4\n",
    "\n",
    "Hm0_bin_size = 0.5/reduce_factor\n",
    "Te_bin_size = 1.0/reduce_factor\n",
    "\n",
    "Hm0_bins = np.arange(0, Hm0_max + Hm0_bin_size, Hm0_bin_size)    \n",
    "Te_bins = np.arange(0, Te_max + Te_bin_size, Te_bin_size)\n",
    "#JM = mhkit.wave.performance.wave_energy_flux_matrix(data_clean.Hm0, data_clean.Te, data_clean.J, 'mean', Hm0_bins, Te_bins)\n",
    "JM = performance.wave_energy_flux_matrix(data_clean.Hm0, data_clean.Te, data_clean.J, 'mean', Hm0_bins, Te_bins)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax = graphics.plot_matrix(JM, ax=ax, show_values=False)\n",
    "#L = mhkit.wave.performance.capture_length(P, data_clean.J) \n",
    "\n",
    "#maep_timeseries = mhkit.wave.performance.mean_annual_energy_production_timeseries(L, data_clean.J)\n",
    "#print(\"MAEP from timeseries = \", maep_timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jmean, xe, ye, bn = stats.binned_statistic_2d(data_clean.Hm0, data_clean.Te, data_clean.J,\n",
    "                                                  statistic='sum',bins=[Te_bins, Hm0_bins])\n",
    "#H, xe,ye = np.histogram2d(data_clean.Hm0, data_clean.Te, bins=[Te_bins, Hm0_bins], density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incident  Wave PowerResults\n",
    "\n",
    "## Full Sea State\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nHours = (data_clean.index[1] - data_clean.index[0]).seconds/3600\n",
    "Total = data_clean.J.sum() * nHours\n",
    "print(f'{Total} (W*hr)/m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Histogram Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Jmean.flatten() #* H.flatten()\n",
    "x = x[~np.isnan(x)]\n",
    "x.sum()/Total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios={}\n",
    "for N in results:\n",
    "    ratios[N] = np.round((results[N].J*len(data_clean)* results[N].weights).sum()/Total,4)\n",
    "    \n",
    "pd.Series(ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAP OF THE AREA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
